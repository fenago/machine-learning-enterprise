{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Machine learning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "rd.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Loading and exploring the dataset\n",
    "\n",
    "First, we use pandas to load the dataset from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pandas.read_csv('./titanic.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the length of the dataset\n",
    "print(\"The dataset has\", len(raw_data), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the columns in the dataset\n",
    "print(\"Columns (features of the dataset)\")\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the labels\n",
    "print(\"Labels\")\n",
    "raw_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining how many passengers survived\n",
    "print(sum(raw_data['Survived']),'passengers survived out of',len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can look at several columns together\n",
    "raw_data[[\"Name\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. Cleaning up the data\n",
    "\n",
    "Now, let's look at how many columns have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cabin column is missing too many values to be useful. Let's drop it altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Cabin column is missing\", sum(raw_data['Cabin'].isna()), \"values out of\",len(raw_data['Cabin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other columns such as Age or Embarked are missing some values, but they can still be useful.\n",
    "\n",
    "For the age column, let's fill in the missing values with the median of all ages.\n",
    "\n",
    "For the Embarked column, let's make a new category called 'U', for Unknown port of embarkment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = raw_data[\"Age\"].median()\n",
    "median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"Age\"] = clean_data[\"Age\"].fillna(median_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"Embarked\"] = clean_data[\"Embarked\"].fillna('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Saving our data for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('./clean_titanic_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Manipulating the features\n",
    "\n",
    "- One-hot encoding\n",
    "- Binning\n",
    "- Feature selection\n",
    "\n",
    "### 13.3.1 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pandas.read_csv('clean_titanic_data.csv')\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the gender feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_columns = pandas.get_dummies(preprocessed_data['Sex'], prefix='Sex')\n",
    "print(gender_columns)\n",
    "embarked_columns = pandas.get_dummies(preprocessed_data[\"Embarked\"], prefix=\"Embarked\")\n",
    "print(embarked_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pandas.concat([preprocessed_data, gender_columns], axis=1)\n",
    "preprocessed_data = pandas.concat([preprocessed_data, embarked_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessed_data.drop(['Sex', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A rule of thumb for when to one-hot encode or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_survived = preprocessed_data[['Pclass', 'Survived']]\n",
    "\n",
    "first_class = class_survived[class_survived['Pclass'] == 1]\n",
    "second_class = class_survived[class_survived['Pclass'] == 2]\n",
    "third_class = class_survived[class_survived['Pclass'] == 3]\n",
    "\n",
    "print(\"In first class\", sum(first_class['Survived'])/len(first_class)*100, \"% of passengers survived\")\n",
    "print(\"In second class\", sum(second_class['Survived'])/len(first_class)*100, \"% of passengers survived\")\n",
    "print(\"In third class\", sum(third_class['Survived'])/len(first_class)*100, \"% of passengers survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_pclass_columns = pandas.get_dummies(preprocessed_data['Pclass'], prefix='Pclass')\n",
    "preprocessed_data = pandas.concat([preprocessed_data, categorized_pclass_columns], axis=1)\n",
    "preprocessed_data = preprocessed_data.drop(['Pclass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.3 Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "categorized_age = pandas.cut(preprocessed_data['Age'], bins)\n",
    "preprocessed_data['Categorized_age'] = categorized_age\n",
    "preprocessed_data = preprocessed_data.drop([\"Age\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagegorized_age_columns = pandas.get_dummies(preprocessed_data['Categorized_age'], prefix='Categorized_age')\n",
    "preprocessed_data = pandas.concat([preprocessed_data, cagegorized_age_columns], axis=1)\n",
    "preprocessed_data = preprocessed_data.drop(['Categorized_age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.3 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessed_data.drop(['Name', 'Ticket', 'PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.5 Saving for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.to_csv('preprocessed_titanic_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4 Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('./preprocessed_titanic_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4.1 Features-labels split and train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop([\"Survived\"], axis=1)\n",
    "labels = data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remark: we fix random_state the end, to make sure we always get the same split\n",
    "features_train, features_validation_test, labels_train, labels_validation_test = train_test_split(\n",
    "    features, labels, test_size=0.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_validation, features_test, labels_validation, labels_test = train_test_split(\n",
    "    features_validation_test, labels_validation_test, test_size=0.5, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features_train))\n",
    "print(len(features_validation))\n",
    "print(len(features_test))\n",
    "print(len(labels_train))\n",
    "print(len(labels_validation))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4.2 Training different models on our dataset\n",
    "\n",
    "We'll train four models:\n",
    "- Logistic regression (perceptron)\n",
    "- Decision tree\n",
    "- Naive Bayes\n",
    "- Support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_model = AdaBoostClassifier()\n",
    "ab_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4.3 Evaluating the models\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scores of the models\")\n",
    "print(\"Logistic regression:\", lr_model.score(features_validation, labels_validation))\n",
    "print(\"Decision tree:\", dt_model.score(features_validation, labels_validation))\n",
    "print(\"Naive Bayes:\", nb_model.score(features_validation, labels_validation))\n",
    "print(\"SVM:\", svm_model.score(features_validation, labels_validation))\n",
    "print(\"Random forest:\", rf_model.score(features_validation, labels_validation))\n",
    "print(\"Gradient boosting:\", gb_model.score(features_validation, labels_validation))\n",
    "print(\"AdaBoost:\", ab_model.score(features_validation, labels_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1-scores of the models:\")\n",
    "\n",
    "lr_predicted_labels = lr_model.predict(features_validation)\n",
    "print(\"Logistic regression:\", f1_score(labels_validation, lr_predicted_labels))\n",
    "\n",
    "dt_predicted_labels = dt_model.predict(features_validation)\n",
    "print(\"Decision Tree:\", f1_score(labels_validation, dt_predicted_labels))\n",
    "\n",
    "nb_predicted_labels = nb_model.predict(features_validation)\n",
    "print(\"Naive Bayes:\", f1_score(labels_validation, nb_predicted_labels))\n",
    "\n",
    "svm_predicted_labels = svm_model.predict(features_validation)\n",
    "print(\"Support Vector Machine:\", f1_score(labels_validation, svm_predicted_labels))\n",
    "\n",
    "rf_predicted_labels = rf_model.predict(features_validation)\n",
    "print(\"Random Forest:\", f1_score(labels_validation, rf_predicted_labels))\n",
    "\n",
    "gb_predicted_labels = gb_model.predict(features_validation)\n",
    "print(\"Gradient boosting:\", f1_score(labels_validation, gb_predicted_labels))\n",
    "\n",
    "ab_predicted_labels = ab_model.predict(features_validation)\n",
    "print(\"AdaBoost:\", f1_score(labels_validation, ab_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4.4 Testing the model\n",
    "\n",
    "Finding the accuracy and the F1-score of the model in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_predicted_test_labels = gb_model.predict(features_test)\n",
    "f1_score(labels_test, gb_predicted_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.5 Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with an rbf kernel\n",
    "\n",
    "print(\"SVM grid search with a radial basis function kernel\")\n",
    "\n",
    "# rbf, C=1, gamma=0.1\n",
    "svm_1_01 = SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "svm_1_01.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=0.1\", svm_1_01.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=1, gamma=1\n",
    "svm_1_1 = SVC(kernel='rbf', C=1, gamma=1)\n",
    "svm_1_1.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=1\", svm_1_1.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=1, gamma=10\n",
    "svm_1_10 = SVC(kernel='rbf', C=1, gamma=10)\n",
    "svm_1_10.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=10\", svm_1_10.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=0.1\n",
    "svm_10_01 = SVC(kernel='rbf', C=10, gamma=0.1)\n",
    "svm_10_01.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=0.1\", svm_10_01.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=1\n",
    "svm_10_1 = SVC(kernel='rbf', C=10, gamma=1)\n",
    "svm_10_1.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=1\", svm_10_1.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=10\n",
    "svm_10_10 = SVC(kernel='rbf', C=10, gamma=10)\n",
    "svm_10_10.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=10\", svm_10_10.score(features_validation, labels_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_parameters = {'kernel': ['rbf'],\n",
    "                  'C': [0.01, 0.1, 1 , 10, 100],\n",
    "                  'gamma': [0.01, 0.1, 1, 10, 100]\n",
    "                }\n",
    "svm = SVC()\n",
    "svm_gs = GridSearchCV(estimator = svm,\n",
    "                      param_grid = svm_parameters)\n",
    "svm_gs.fit(features_train, labels_train)\n",
    "\n",
    "svm_winner = svm_gs.best_estimator_\n",
    "svm_winner\n",
    "\n",
    "svm_winner.score(features_validation, labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.6 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pandas.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "test_data = test_data.drop('Cabin', axis=1)\n",
    "test_data[\"Age\"] = test_data[\"Age\"].fillna(28.0)\n",
    "\n",
    "# Catch! The test data has one missing fare. Let's fix that\n",
    "average_fare = test_data[\"Fare\"].mean()\n",
    "test_data['Fare'] = test_data['Fare'].fillna(average_fare)\n",
    "\n",
    "# Preprocessing the data\n",
    "test_gender_columns = pandas.get_dummies(test_data['Sex'], prefix='Sex')\n",
    "test_embarked_columns = pandas.get_dummies(test_data[\"Embarked\"], prefix=\"Embarked\")\n",
    "test_data = pandas.concat([test_data, test_gender_columns], axis=1)\n",
    "test_data = pandas.concat([test_data, test_embarked_columns], axis=1)\n",
    "test_data = test_data.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "# Another small catch, the test data has no missing 'Embarked' fields. Therefore, the processed test data will not\n",
    "# have an 'Embarked_Q' column. We need to artificially add one filled with zeros.\n",
    "test_data['Embarked_U'] = pandas.DataFrame([0 for i in range(len(test_data))])\n",
    "\n",
    "test_categorized_pclass_columns = pandas.get_dummies(test_data['Pclass'], prefix='Pclass')\n",
    "test_data = pandas.concat([test_data, test_categorized_pclass_columns], axis=1)\n",
    "test_data = test_data.drop(['Pclass'], axis=1)\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "test_categorized_age = pandas.cut(test_data['Age'], bins)\n",
    "test_data['Categorized_age'] = categorized_age\n",
    "test_data = test_data.drop([\"Age\"], axis=1)\n",
    "\n",
    "test_cagegorized_age_columns = pandas.get_dummies(test_data['Categorized_age'], prefix='Categorized_age')\n",
    "test_data = pandas.concat([test_data, test_cagegorized_age_columns], axis=1)\n",
    "test_data = test_data.drop(['Categorized_age'], axis=1)\n",
    "\n",
    "test_data = test_data.drop(['Name', 'Ticket', 'PassengerId'], axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to check how many survivors were predicted by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "sum(lr_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "sum(dt_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "sum(nb_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine\n",
    "sum(svm_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "sum(rf_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting\n",
    "sum(gb_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "sum(ab_model.predict(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the three strongest models in terms of accuracy were random forests, gradient boosting, and adaboost, and they predicted that 154, 156, and 155 passengers survived out of the 418 in the test set, a good estimate for the number of survivors is the average of these three predictions, or 155."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
